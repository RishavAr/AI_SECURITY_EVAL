# 🔐 AI Security Eval – Benchmarking LLMs on Cybersecurity Payloads

## 📌 Overview
This project evaluates Large Language Models (LLMs) like GPT-3.5, GPT-4o, and GPT-4o-mini on their ability to detect and classify malicious vs. benign inputs.
It leverages the PayloadsAllTheThings repository as a rich source of real-world attack payloads and augments them with benign samples to form a benchmark dataset called CyberEvalBench.


The system provides:
⚡ Automated dataset generation from PayloadsAllTheThings.
🧪 Evaluation pipeline for GPT models via OpenAI API.
📊 Scoring & reporting to measure detection accuracy.
🎯 A foundation for research on secure LLM systems & red-teaming AI.


