# ğŸ” AI Security Eval â€“ Benchmarking LLMs on Cybersecurity Payloads

## ğŸ“Œ Overview
This project evaluates Large Language Models (LLMs) like GPT-3.5, GPT-4o, and GPT-4o-mini on their ability to detect and classify malicious vs. benign inputs.
It leverages the PayloadsAllTheThings repository as a rich source of real-world attack payloads and augments them with benign samples to form a benchmark dataset called CyberEvalBench.


The system provides:
âš¡ Automated dataset generation from PayloadsAllTheThings.
ğŸ§ª Evaluation pipeline for GPT models via OpenAI API.
ğŸ“Š Scoring & reporting to measure detection accuracy.
ğŸ¯ A foundation for research on secure LLM systems & red-teaming AI.


